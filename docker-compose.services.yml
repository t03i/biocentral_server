version: '3.8'

services:
  # Keep existing legacy service unchanged
  legacy-service:
    build: .  # Uses existing Dockerfile
    ports:
      - "9540:9540"
    environment:
      - REDIS_JOBS_HOST=redis-jobs
      - REDIS_JOBS_PORT=6379
    depends_on:
      - redis-jobs
    volumes:
      - ./storage:/app/storage
      - ./logs:/app/logs

  # Triton Inference Server for ProtT5 embeddings
  triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC 
      - "8002:8002"  # Metrics
    volumes:
      - ./services/model-repository:/models:ro
    command: >
      tritonserver 
      --model-repository=/models 
      --strict-model-config=false
      --log-verbose=1
      --allow-grpc=true
      --allow-http=true
      --allow-metrics=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # New ProtT5 embedding service with Triton integration
  embedding-service:
    build: 
      context: ./services/embeddings-service
    environment:
      - REDIS_URL=redis://redis-jobs:6379
      - TRITON_URL=triton:8001
    depends_on:
      - redis-jobs
      - triton
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8001/embeddings/health', timeout=5).raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # New prediction service with Triton integration
  prediction-service:
    build:
      context: ./services/prediction-service
    environment:
      - TRITON_URL=triton:8001
      - EMBEDDINGS_SERVICE_URL=http://embedding-service:8001
    depends_on:
      - triton
      - embedding-service
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8002/health', timeout=5).raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s

  # Caddy reverse proxy
  caddy:
    image: caddy:2-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
      - ./logs/caddy:/var/log/caddy
    depends_on:
      - legacy-service
      - embedding-service
      - prediction-service

  # Redis for queues and embedding caching
  redis-jobs:
    image: redis:7-alpine
    ports: ["6379:6379"]
    command: >
      redis-server 
      --maxmemory 4gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --appendonly yes
      --appendfsync everysec
    volumes: ["redis_data:/data"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

volumes:
  redis_data:
  caddy_data:
  caddy_config:
